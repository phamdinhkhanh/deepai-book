{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-interview",
   "metadata": {},
   "source": [
    "# 1. Đại số tuyến tính\n",
    "\n",
    "## 1.1. Số vô hướng (scalar)\n",
    "\n",
    "Trong cuộc sống hàng ngày chúng ta sẽ gặp rất nhiều số vô hướng (scalar). Giá trị của tiền nhà tháng này mà bạn phải trả cho chủ nhà là một số vô hướng. Bạn vừa thực hiện một bài kiểm tra toán, bạn được 9 điểm thì điểm số này là một số vô hướng,.... Tóm lại số vô hướng là một con số cụ thể. Số vô hướng sẽ khác với biến số vì biến số có thể nhận nhiều giá trị trong khi số vô hướng chỉ nhận một giá trị duy nhất. Ví dụ, khi biểu diễn giá nhà $y$ theo diện tích $x$ theo phương trình $y=20x + 200$ thì các số  vô hướng là $20, 200$ và các biến là $x, y$.\n",
    "\n",
    "Để khởi tạo một số vô hướng, chúng ta sẽ sử dụng tensor\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor(20)\n",
    "b = torch.tensor(200)\n",
    "print(\"diện tích x = 50 --> giá nhà y = a*50+b = \", a*50+b)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "a = np.array(20)\n",
    "b = np.array(200)\n",
    "print(\"diện tích x = 50 --> giá nhà y = a*50+b = \", a*50+b)\n",
    ":::\n",
    "::::\n",
    "\n",
    "Số vô hướng có thể được coi như hằng số trong một phương trình. Chúng ta có thể thực hiện các phép toán cộng/trừ/nhân/chia với số vô hướng như với hằng số.\n",
    "\n",
    "## 1.2. Véc tơ\n",
    "\n",
    "Véc tơ là một khái niệm cơ bản nhất của toán học. Chúng ta có thể coi véc tơ là một tập hợp nhiều giá trị của số vô hướng. Véc tơ thường biểu diễn một đại lượng cụ thể trên thực tiễn. Ví dụ như diện tích của các căn nhà là một véc tơ, số lượng phòng ngủ cũng là một véc tơ. Véc tơ có độ dài đặc trưng chính bằng số lượng các phần tử trong nó. Để khởi tạo một véc tơ, trong pytorch chúng ta bao các giá trị của nó trong dấu ngoặc vuông.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "x = torch.tensor([1, 1.2, 1.5, 1.8, 2])\n",
    "x\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "x = np.array([1, 1.2, 1.5, 1.8, 2])\n",
    "x\n",
    "::::\n",
    "\n",
    "### 1.2.1. Các thuộc tính của véc tơ\n",
    "\n",
    "Một véc tơ sẽ có độ dài và định dạng dữ liệu xác định. Ngoài ra nếu coi một biến số là một véc tơ thì trong thống kê mô tả chúng ta sẽ quan tâm tới tổng, trung bình, phương sai, giá trị lớn nhất, nhỏ nhất.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Độ dài\n",
    "print(\"length of vector: \", x.size()) # or len(x)\n",
    "\n",
    "# Định dạng của véc tơ\n",
    "print(\"vector type: \", x.dtype)\n",
    "\n",
    "# Tổng của các phần tử \n",
    "print(\"sum of vector: \", x.sum())\n",
    "\n",
    "# Trung bình các phần tử\n",
    "print(\"mean of vector: \", x.mean())\n",
    "\n",
    "# Giá trị nhỏ nhất\n",
    "print(\"min of vector: \", x.min())\n",
    "\n",
    "# Giá trị lớn nhất\n",
    "print(\"max of vector: \", x.max())\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Độ dài\n",
    "print(\"length of vector: \", x.shape) # or len(x)\n",
    "\n",
    "# Định dạng của véc tơ\n",
    "print(\"vector type: \", x.dtype)\n",
    "\n",
    "# Tổng của các phần tử \n",
    "print(\"sum of vector: \", x.sum())\n",
    "\n",
    "# Trung bình các phần tử\n",
    "print(\"mean of vector: \", x.mean())\n",
    "\n",
    "# Giá trị nhỏ nhất\n",
    "print(\"min of vector: \", x.min())\n",
    "\n",
    "# Giá trị lớn nhất\n",
    "print(\"max of vector: \", x.max())\n",
    ":::\n",
    "::::\n",
    "\n",
    "### 1.2.2. Các phép tính trên véc tơ\n",
    "\n",
    "Chúng ta có thể thực hiện các phép tính trên véc tơ như phép cộng, trừ, tích vô hướng, tích có hướng giữa hai véc tơ.. Lưu ý là chúng phải có cùng độ dài. Trong khuôn khổ cuốn sách này, các véc tơ sẽ được ký hiệu là một ký tự chữ thường in đậm như $\\mathbf{x}, \\mathbf{y}, \\mathbf{z}$. Ngoài ra $\\mathbf{x}\\in \\mathbb{R}^{n}$ là véc tơ số thực có độ dài $n$.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "x = torch.tensor([1, 2, 1.5, 1.8, 1.9])\n",
    "y = torch.tensor([1.1, 2.2, 1.2, 1.6, 1.7])\n",
    "print(\"x + y: \", x + y)\n",
    "print(\"x - y: \", x - y)\n",
    "print(\"x * y: \", x * y)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 1.5, 1.8, 1.9])\n",
    "y = np.array([1.1, 2.2, 1.2, 1.6, 1.7])\n",
    "print(\"x + y: \", x + y)\n",
    "print(\"x - y: \", x - y)\n",
    "print(\"x * y: \", x * y)\n",
    ":::\n",
    "::::\n",
    "\n",
    "Véc tơ có thể thực hiện các phép cộng, trừ, nhân, chia với một số vô hướng. Giá trị thu được là một véc tơ cùng kích thước mà mỗi phần tử của nó là kết quả được thực hiện trên từng phần tử của véc tơ với số vô hướng đó.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "x = torch.tensor([1, 2, 1.5, 1.8, 1.9])\n",
    "print(\"x + 5: \", x + 5)\n",
    "print(\"x - 5: \", x - 5)\n",
    "print(\"x * 5: \", x * 5)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "x = np.array([1, 2, 1.5, 1.8, 1.9])\n",
    "print(\"x + 5: \", x + 5)\n",
    "print(\"x - 5: \", x - 5)\n",
    "print(\"x * 5: \", x * 5)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "## 1.3. Ma trận\n",
    "\n",
    "Véc tơ là đại lượng một chiều nên nó chỉ có thể biểu diễn cho một biến. Trong trường hợp chúng ta cần biểu diễn cho nhiều biến thì sẽ cần tới đại lượng hai chiều là ma trận. Ma trận được ký hiệu bởi một chữ cái in đậm $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ là một ma trận số thực có $m$ dòng và $n$ cột.\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\\begin{bmatrix} \n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\ \n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \n",
    "\\end{bmatrix}.\\end{split}$$\n",
    "\n",
    "Để xác định một phần tử bất kỳ thuộc dòng thứ $i$, cột thứ $j$ của ma trận $\\mathbf{A}$ ta ký hiệu chúng là $\\mathbf{A}_{ij}$. Véc tơ dòng thứ $i$ sẽ là $\\mathbf{A}_{i:}$ và véc tơ cột thứ $j$ sẽ là $\\mathbf{A}_{:j}$. Để đơn giản hoá ta qui ước $\\mathbf{A}_{j}$ là véc tơ cột $j$ và $\\mathbf{A}^{(i)}$ là véc tơ dòng $i$.\n",
    "\n",
    "### 1.3.1. Các ma trận đặc biệt\n",
    "\n",
    "* Ma trận vuông: Ma trận vuông là ma trận có số dòng bằng số cột. Ma trận vuông rất quan trọng vì khi tìm nghiệm cho hệ phương trình, từ ma trận vuông ta có thể chuyển sang ma trận tam giác. Ma trận vuông cũng là ma trận có thể tính được giá trị định thức. Tóm lại ma trận $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ vuông nếu $m=n$. \n",
    "\n",
    "* Ma trận đơn vị: Là ma trận có đường chéo chính bằng 1, các phần tử còn lại bằng $0$. Ví dụ ma trận đơn vị kích thước $3 \\times 3$ được ký hiệu là $\\mathbf{I}_3$ có gía trị là:\n",
    "\n",
    "$$\\begin{split}\\mathbf{I}_3=\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "\\end{split}$$\n",
    "\n",
    "Tóm lại, $\\mathbf{A}$ là ma trận đơn vị nếu nó là ma trận vuông và $a_{ij} = 1$ nếu $i=j$ và $a_{ij} = 0$ nếu $i \\neq j$.\n",
    "\n",
    "* Ma trận đường chéo: Là ma trận có các phần tử trên đường chéo chính khác 0 và các phần tử còn lại bằng 0. Ví dụ về ma trận đường chéo:\n",
    "\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 2 & 0 \\\\ \n",
    "0 & 0 & 3 \n",
    "\\end{bmatrix}\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "* Ma trận chuyển vị: $\\mathbf{B}$ là ma trận chuyển vị của $\\mathbf{A}$ nếu $b_{ij} = a_{ji}$ với mọi $i, j$. Dễ hiểu hơn, tức là mọi dòng của ma trận $A$ sẽ là cột của ma trận $\\mathbf{B}$. Ví dụ:\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix} \n",
    "1 & 2 & 3 \\\\ \n",
    "3 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{split}, \\begin{split}\\mathbf{B}=\n",
    "\\begin{bmatrix} \n",
    "1 & 3 \\\\\n",
    "2 & 2 \\\\ \n",
    "3 & 1\\end{bmatrix}\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "Ký hiệu chuyển vị của ma trận $\\mathbf{A}$ là $\\mathbf{A}^{\\intercal}$\n",
    "\n",
    "### 1.3.2. Các thuộc tính của ma trận\n",
    "\n",
    "Một ma trận được đặc trưng bởi dòng và cột.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [3, 2, 1]])\n",
    "\n",
    "# shape của matrix A\n",
    "A.size()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], \n",
    "              [3, 2, 1]])\n",
    "\n",
    "# shape của matrix A\n",
    "A.shape\n",
    ":::\n",
    "::::\n",
    "\n",
    "### 1.3.3. Các phép tính trên ma trận\n",
    "\n",
    "Hai ma trận có cùng kích thước chúng ta có thể thực hiện các phép cộng, trừ, tích hadamard (hoặc elementi-wise). Ma trận thu được cũng có cùng kích thước và các phần tử của nó được tính dựa trên các phần tử có cùng vị trí trên cả hai ma trận $\\mathbf{A}$ và $\\mathbf{B}$.\n",
    "\n",
    "**Tích hadamard hoặc element-wise**\n",
    "\n",
    "$$\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}\\end{split}\n",
    "$$\n",
    "\n",
    "Trên pytorch chúng ta có thể tính tích hadamard của hai ma trận đơn giản như sau:\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [3, 2, 1]])\n",
    "\n",
    "B = torch.tensor([[2, 1, 2], \n",
    "                  [1, 3, 0]])\n",
    "\n",
    "A*B\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], \n",
    "              [3, 2, 1]])\n",
    "\n",
    "B = np.array([[2, 1, 2], \n",
    "             [1, 3, 0]])\n",
    "\n",
    "A*B\n",
    ":::\n",
    "::::\n",
    "\n",
    "Tương tự với các phép cộng và trừ\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "print(\"A-B: \\n\", A-B)\n",
    "print(\"A+B: \\n\", A+B)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "print(\"A-B: \\n\", A-B)\n",
    "print(\"A+B: \\n\", A+B)\n",
    ":::\n",
    "::::\n",
    "\n",
    "**Tích thông thường**: Tích thông thường giữa hai ma trận $\\mathbf{A}$ có kích thước $m \\times n$ và $\\mathbf{B}$ có kích thước $n \\times p$ là một ma trận có kích thước $m \\times p$. Ma trận output $\\mathbf{C}$ có giá trị tại phần tử $c_{ij} = \\mathbf{A}^{(i)} \\mathbf{B}_{j}$ (dòng thứ $i$ của ma trận $\\mathbf{A}$ nhân với cột thứ $j$ của ma trận $\\mathbf{B}$).\n",
    "\n",
    "$$\n",
    "\\begin{split}\\mathbf{A}_{m \\times n} \\mathbf{B}_{n \\times p} =\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{A}^{(1)}  \\mathbf{B}_{1} & \\mathbf{A}^{(1)}  \\mathbf{B}_{2} & \\dots  & \\mathbf{A}^{(1)}  \\mathbf{B}_{p} \\\\\n",
    "    \\mathbf{A}^{(2)}  \\mathbf{B}_{1} & \\mathbf{A}^{(2)}  \\mathbf{B}_{2} & \\dots  & \\mathbf{A}^{(2)}  \\mathbf{B}_{p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{A}^{(m)}  \\mathbf{B}_{1} & \\mathbf{A}^{(m)}  \\mathbf{B}_{2} & \\dots  & \\mathbf{A}^{(m)}  \\mathbf{B}_{p} \\\\\n",
    "\\end{bmatrix}\\end{split}_{m \\times p}\n",
    "$$\n",
    "\n",
    "Chắc các bạn còn nhớ $\\mathbf{A}^{(i)}$ là véc tơ dòng và $\\mathbf{A}_{j}$ là véc tơ cột.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [3, 2, 1]])\n",
    "\n",
    "B = torch.tensor([[2, 1], \n",
    "                  [1, 3],\n",
    "                  [1, 1]])\n",
    "\n",
    "A@B\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], \n",
    "              [3, 2, 1]])\n",
    "\n",
    "B = np.array([[2, 1], \n",
    "              [1, 3],\n",
    "              [1, 1]])\n",
    "\n",
    "A@B\n",
    ":::\n",
    "::::\n",
    "\n",
    "### 1.3.4. Truy cập thành phần\n",
    "\n",
    "Chúng ta có thể truy cập vào các thành phần của ma trận $\\mathbf{A}$ dựa theo các chỉ số slice index. Chúng ta có thể tổng hợp kiến thức về truy cập thành phần trong bản sau:\n",
    "\n",
    "| Cú pháp      | Mô tả |\n",
    "| ----------- | ----------- |\n",
    "| :n      | lấy n index đầu tiên từ [0, 1, ..., n-1]       |\n",
    "| -n:   | lấy n index cuối cùng từ [len-n, ..., len-1]        |\n",
    "| i:j   | lấy các index từ [i, i+1, ..., j-1]        |\n",
    "| ::2   | lấy các index chẵn liên tiếp [0, 2, 4 ..., int(len/2)*2]        |\n",
    "| ::k   | lấy các index cách đều và chia hết cho k một cách liên tiếp [0, k, 2k, ..., int(len/k)*k]        |\n",
    "| :   | lấy toàn bộ index        |\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [3, 2, 1],\n",
    "                  [4, 2, 2]])\n",
    "\n",
    "# Truy cập ma trận con từ 2 dòng đầu tiên và 2 cột đầu tiên.\n",
    "A[:2, :2]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], \n",
    "              [3, 2, 1],\n",
    "              [4, 2, 2]])\n",
    "\n",
    "# Truy cập ma trận con từ 2 dòng đầu tiên và 2 cột đầu tiên.\n",
    "A[:2, :2]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập ma trận con từ 2 dòng cuối cùng và 2 cột đầu tiên\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận con từ 2 dòng cuối cùng và 2 cột đầu tiên\n",
    "A[-2:, :2]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận con từ 2 dòng cuối cùng và 2 cột đầu tiên\n",
    "A[-2:, :2]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập véc tơ con từ dòng thứ 2 và 2 cột cuối cùng.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập véc tơ con từ dòng thứ 2 và 2 cột cuối cùng.\n",
    "print(A[2, -2:])\n",
    "\n",
    "# Hoặc\n",
    "A[2:3, -2:][0]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập véc tơ con từ dòng thứ 2 và 2 cột cuối cùng.\n",
    "print(A[2, -2:])\n",
    "\n",
    "# Hoặc\n",
    "A[2:3, -2:][0]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập ma trận có các dòng chẵn\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận có các dòng chẵn\n",
    "A[::2, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận có các dòng chẵn\n",
    "A[::2, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập một index cụ thể, ví dụ dòng 0, 2 của ma trận\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập một index cụ thể , ví dụ dòng 0, 2 của ma trận\n",
    "A.index_select(0, torch.tensor([0, 2]))\n",
    "# Trong công thức trên 0 là chiều mà ta sẽ lấy, tensor([0, 2]) là các index ta sẽ lấy từ chiều 0.\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập một index cụ thể , ví dụ dòng 0, 2 của ma trận\n",
    "A[[0, 2], :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "## 1.4. Tensor\n",
    "\n",
    "Tensor là một định dạng đặc biệt được nghĩ ra bởi google. Nó tổng quát hơn so với ma trận vì có thể biểu diễn được các không gian với số chiều tuỳ ý. Chẳng hạn trong xử lý ảnh chúng ta có một bức ảnh với kích thước là $W \\times H \\times C$ lần lượt $W, H, C$ là chiều _width, height và channels_. Thông thường $C = 1$ hoặc $3$ tuỳ theo ảnh là ảnh xám hay ảnh màu. Trong huấn luyện mô hình phân loại ảnh thì các đầu vào được kết hợp theo mini-batch nên sẽ có thêm một chiều về batch_size. Do đó input có kích thước là $N \\times W \\times H \\times C$.\n",
    "\n",
    "### 1.4.1. Các thuộc tính của tensor\n",
    "\n",
    "Một tensor được đặc trưng bởi kích thước các chiều, số lượng chiều, định dạng dữ liệu của tensor.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "A = torch.tensor([[[1, 2, 3], \n",
    "                  [3, 2, 1]],\n",
    "                  [[2, 1, 2], \n",
    "                  [1, 3, 0]]])\n",
    "\n",
    "# Kích thước của tensor\n",
    "print(\"shape of A: \" , A.size())\n",
    "\n",
    "# Số chiều \n",
    "print(\"total dim: \", A.ndim)\n",
    "\n",
    "# Định dạng dữ liệu\n",
    "print(\"dtype: \", A.dtype)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "A = np.array([[[1, 2, 3], \n",
    "              [3, 2, 1]],\n",
    "              [[2, 1, 2], \n",
    "              [1, 3, 0]]])\n",
    "\n",
    "# Kích thước của array\n",
    "print(\"shape of A: \" , A.shape)\n",
    "\n",
    "# Số chiều \n",
    "print(\"total dim: \", A.ndim)\n",
    "\n",
    "# Định dạng dữ liệu\n",
    "print(\"dtype: \", A.dtype)\n",
    ":::\n",
    "::::\n",
    "\n",
    "### 1.4.2. Các phép tính trên tensor\n",
    "\n",
    "**Tích thông thường giữa 2 tensors**: Nếu tensor $\\mathbf{A}$ có kích thước $m \\times n \\times p$ và tensor $\\mathbf{B}$ có kích thước $n \\times p \\times q$ thì tích giữa chúng có kích thước là $m \\times n \\times q$. Trên python chúng ta sử dụng ký hiệu `@` để đại diện cho tích thông thường.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "A = torch.randn([2, 3, 4])\n",
    "B = torch.randn([2, 4, 2])\n",
    "\n",
    "# Tích giữa 2 tensor\n",
    "(A@B).size()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.randn(2, 3, 4)\n",
    "B = np.random.randn(2, 4, 2)\n",
    "\n",
    "# Tích giữa 2 array\n",
    "(A@B).shape\n",
    ":::\n",
    "::::\n",
    "\n",
    "Ngoài ra chúng ta có thể tính **tích hadamard giữa 2 tensors** $\\mathbf{A}$ và $\\mathbf{B}$ được ký hiệu bằng dấu `*` như sau:\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "A = torch.randn([2, 3, 4])\n",
    "B = torch.randn([2, 3, 4])\n",
    "\n",
    "# Tích hadamard giữa 2 tensor\n",
    "(A*B).size()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.randn(2, 3, 4)\n",
    "B = np.random.randn(2, 3, 4)\n",
    "\n",
    "# Tích hadamard giữa 2 array\n",
    "(A*B).shape\n",
    ":::\n",
    "::::\n",
    "\n",
    "Chúng ta cũng có thể thực hiện các phép cộng, trừ giữa các tensor cùng kích thước.\n",
    "\n",
    "Phép cộng\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Phép cộng\n",
    "(A+B).size()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Phép cộng\n",
    "(A+B).shape\n",
    ":::\n",
    "::::\n",
    "\n",
    "Phép trừ\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Phép trừ\n",
    "(A-B).size()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Phép trừ\n",
    "(A-B).shape\n",
    ":::\n",
    "::::\n",
    "\n",
    "**Truy cập thành phần**: Để truy cập vào một mảng thành phần của $\\mathbf{A}$ chúng ta sẽ cần khai báo vị trí indices của chúng trên mỗi chiều của ma trận $\\mathbf{A}$. Cách truy cập cũng hoàn toàn tương tự như đối với ma trận.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "# Khởi tạo ma trận A kích thước m, n, p = 2, 3, 4\n",
    "A = torch.randn([2, 3, 4])\n",
    "\n",
    "# Truy cập ma trận đầu tiên \n",
    "A[:1, :, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "# Khởi tạo ma trận A kích thước m, n, p = 2, 3, 4\n",
    "A = np.random.randn(2, 3, 4)\n",
    "\n",
    "# Truy cập ma trận đầu tiên \n",
    "A[:1, :, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập ma trận đầu tiên và chỉ lấy dòng từ 1 tới 3\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận đầu tiên và chỉ lấy dòng từ 1 tới 3\n",
    "A[0][1:3, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập ma trận đầu tiên và chỉ lấy dòng từ 1 tới 3\n",
    "A[0][1:3, :]\n",
    ":::\n",
    "::::\n",
    "\n",
    "Truy cập tương ứng với các chiều m, n, p lần lượt index đầu tiên, 2 index đầu tiên, và index thứ 3.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# Truy cập tương ứng với các chiều m, n, p lần lượt index đầu tiên, 2 index đầu tiên, và index thứ 3.\n",
    "A[:1, :2, 3]\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# Truy cập tương ứng với các chiều m, n, p lần lượt index đầu tiên, 2 index đầu tiên, và index thứ 3.\n",
    "A[:1, :2, 3]\n",
    ":::\n",
    "::::\n",
    "\n",
    "## 1.5. Tích giữa một ma trận với véc tơ\n",
    "\n",
    "Bản chất của phép nhân một ma trận với một véc tơ là một **phép biến hình**. Giả sử bạn có ma trận $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ và véc tơ $\\mathbf{x} \\in \\mathbb{R}^{n}$. Khi đó tích giữa ma trận $\\mathbf{A}$ với véc tơ $\\mathbf{x}$ là một véc tơ $\\mathbf{y}$ có kích thước mới là $\\mathbf{y} \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix} \\mathbf{x} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{x} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\mathbf{x} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{bmatrix} = \\mathbf{y}$$\n",
    "\n",
    "Trong đó $\\mathbf{a}_i^{\\intercal}$ là véc tơ dòng thứ $i$ của ma trận $\\mathbf{A}$. Như vậy thông qua ma trận $\\mathbf{A}$ chúng ta đã biến đổi véc tơ $\\mathbf{x}$ từ không gian $n$ chiều sang véc tơ $\\mathbf{y}$ trong không gian $m$ chiều. Đây là một định lý rất quan trọng vì bạn sẽ gặp nó thường xuyên trong mạng nơ ron để giảm chiều dữ liệu, trong phân tích suy biến, trong phép xoay ảnh và đặc biệt nhất là trong hồi qui tuyến tính.\n",
    "\n",
    "Giả sử bạn đã biết được các biến đầu vào gồm: diện tích và số phòng ngủ như các dòng của ma trận bên dưới:\n",
    "\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "X = torch.tensor([[100, 120, 80, 90, 105, 95], \n",
    "                  [2, 3, 2, 2, 3, 2]])\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[100, 120, 80, 90, 105, 95], \n",
    "              [2, 3, 2, 2, 3, 2]])\n",
    ":::\n",
    "::::\n",
    "\n",
    "Và hệ số hồi qui tương ứng với với chúng lần lượt là $\\mathbf{w} = (10, 100)$. Khi đó giá nhà có thể được ước lượng bằng tích $\\mathbf{y} = \\mathbf{X}^{\\top}\\mathbf{w}$\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "w = torch.tensor([[10], [100]])\n",
    "y = X.T@w\n",
    "y\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "w = np.array([[10], [100]])\n",
    "y = X.T@w\n",
    "y\n",
    ":::\n",
    "::::\n",
    "\n",
    "## 1.6. Tích vô hướng\n",
    "\n",
    "Tích vô hướng giữa hai véc tơ $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$ có cùng kích thước là một số vô hướng được ký hiệu là $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ hoặc $\\mathbf{x}^{\\top}\\mathbf{y}$ có công thức như sau:\n",
    "\n",
    "$$\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{d} x_i y_i$$\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([2, 3, 4])\n",
    "x.dot(y)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 4])\n",
    "x.dot(y)\n",
    ":::\n",
    "::::\n",
    "\n",
    "Tích vô hướng rất quan trọng. Đây cũng là lý do mà tôi phải tách chúng thành một mục riêng. Bạn có thể bắt gặp tích vô hướng rất nhiều trong machine learning. Bên dưới là một số tình huống thường gặp:\n",
    "\n",
    "- Tích vô hướng có thể được sử dụng để tính giá trị ước lượng của phương trình hồi qui tuyến tính. Ví dụ nếu bạn biết giá nhà được biểu diễn theo diện tích $x_1$ và số phòng ngủ $x_2$ theo công thức: \n",
    "\n",
    "$$y=20x_1 + 10x_2+ 200$$\n",
    "\n",
    "Thì một cách khái quát bạn có thể ước lượng $y$ theo tích vô hướng giữa véc tơ đầu vào $\\mathbf{x}^{\\top} = (x_1, x_2, 1)$ và véc tơ hệ số $\\mathbf{w} = (20, 10, 200)$ như sau:\n",
    " \n",
    "$$\\hat{y} = \\mathbf{x}^{\\top}\\mathbf{w}$$\n",
    "\n",
    "- Tích vô hướng cũng được sử dụng để tính trung bình có trọng số của $\\mathbf{x}$:\n",
    "\n",
    "$$\\bar{\\mathbf{x}} = \\sum_{i=1}^{n} x_i q_i= \\mathbf{x}^{\\top}\\mathbf{q}$$\n",
    "với $\\sum_{i=1}^{n} q_i= 1$\n",
    "\n",
    "- Ngoài ra, chắc hẳn bạn còn nhớ cách tính cos giữa hai véc tơ $\\mathbf{x}$ và $\\mathbf{y}$ sẽ bằng tích vô hướng giữa hai véc tơ được chuẩn hoá theo norm chuẩn bậc 2.\n",
    "\n",
    "$$\\cos({\\mathbf{x}, \\mathbf{y}}) = \\frac{\\sum_{i=1}^{d} x_i y_i}{ \\sqrt{\\sum_{i=1}^{d} x_i^2} \\sqrt{\\sum_{i=1}^d y_i^2}}= \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\mathbf{||x||_2}\\mathbf{||y||_2}}\n",
    "$$\n",
    "\n",
    "Khái niệm về norm chuẩn bậc 2 cũng là một kiến thức rất quan trọng. Mình sẽ giúp các bạn tìm hiểu bên dưới.\n",
    "\n",
    "\n",
    "## 1.7. Khái niệm chuẩn\n",
    "\n",
    "Chuẩn là một khái niệm liên quan đến véc tơ. Hay nói chính xác hơn nó là một độ đo trên véc tơ để so sánh các véc tơ với nhau. Cụ thể hơn: \n",
    "\n",
    "$f(\\mathbf{x})$ là một phép ánh xạ từ véc tơ sang một đại lượng vô hướng $\\mathbb{R}^{d} \\mapsto \\mathbb{R}$ nếu nó thoả mãn các tính chất.\n",
    "\n",
    "1. Tính chất co dãn: \n",
    "\n",
    "$$\\alpha f(\\mathbf{x}) = f(\\alpha\\mathbf{x})$$\n",
    "\n",
    "Như vậy khi bạn phóng đại lên véc tơ $\\alpha$ lần thì giá trị chuẩn của nó cũng phóng đại lên $\\alpha$ lần.\n",
    "\n",
    "2. Bất đẳng thức tam giác: \n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\n",
    "  \\mathbf{y}\n",
    ")$$\n",
    "\n",
    "Nếu ta coi $\\mathbf{x}$ như là véc tơ cạnh và $f(\\mathbf{x})$ như là độ dài cạnh của một tam giác thì $f(\\mathbf{x}), f(\\mathbf{y})$ là độ dài của 2 cạnh bất kỳ và tổng của chúng sẽ lớn hơn độ dài cạnh còn lại $f(\\mathbf{x} + \\mathbf{y})$.\n",
    "\n",
    "3. Tính chất không âm: \n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0, \\forall \\mathbf{x}$$\n",
    "\n",
    "Tính chất này là hiển nhiên vì đã là độ đo thì không được âm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-mercy",
   "metadata": {
    "id": "2km6PEUFzmDO"
   },
   "source": [
    "Trong machine learning các bạn sẽ thường xuyên gặp một số chuẩn chính là chuẩn bậc 2 \n",
    "\n",
    "$$L_{2} = \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2 }$$\n",
    "\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "x = torch.randn(10)\n",
    "torch.norm(x, p=2)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "np.linalg.norm(x, ord=2)\n",
    ":::\n",
    "::::\n",
    "\n",
    "Ta nhận thấy hàm MSE đo lường sai số giữa giá trị dự báo và thực tế trong phương trình hồi qui tuyến tính cũng là một dạng chuẩn bậc 2.\n",
    "\n",
    "Chuẩn bậc 1:\n",
    "\n",
    "$$L_{1} = \\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right| $$\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "print(torch.norm(x, p=1))\n",
    "# hoặc \n",
    "torch.abs(x).sum()\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "print(np.linalg.norm(x, ord=1))\n",
    "# hoặc \n",
    "np.abs(x).sum()\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "Trong hồi qui tuyến tính thì chuẩn bậc 1 đo lường sai số tuyệt đối giữa giá trị dự báo và giá trị thực tế. Tuy nhiên nó ít được sử dụng hơn so với chuẩn bậc 2 như là một loss function vì giá trị của nó có đạo hàm không liên tục. Điều này dẫn tới việc huấn luyện mô hình không ổn định. Tuy nhiên nó cũng khá thường xuyên được sử dụng trong các mô hình deep learning chẳng hạn như GAN.\n",
    "\n",
    "Cả hai chuẩn trên đều là trường hợp cụ thể của chuẩn bậc $p$ (ký hiệu $L_{p}$) tổng quát hơn có công thức như sau:\n",
    "\n",
    "$$L_{p} = \\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}$$\n",
    "\n",
    "Để cả 3 điều kiện về chuẩn được thoả mãn thì chúng ta cần có $p \\geq 1$.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "# chuẩn p bất kỳ >= 1, chẳng hạn p=1.5\n",
    "torch.norm(x, p=1.5)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "# chuẩn p bất kỳ >= 1, chẳng hạn p=1.5\n",
    "np.linalg.norm(x, ord=1.5)\n",
    ":::\n",
    "::::\n",
    "\n",
    "## 1.8. Định thức và các tính chất của định thức\n",
    "\n",
    "Giả sử ta có một ma trận vuông $\\mathbf{M}$ như sau:\n",
    "\n",
    "$$\\mathbf{M}=\\begin{bmatrix} \n",
    "m^{1}_{1} & m^{1}_{2} & \\dots & m^{1}_{n}\\\\ \n",
    "m^{2}_{1} & m^{2}_{2} & \\dots & m^{2}_{n}\\\\ \n",
    "\\dots & \\dots & \\ddots & \\dots\\\\ \n",
    "m^{n}_{1} & m^{n}_{2} & \\dots & m^{n}_{n}\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Định thức của một ma trận là một giá trị được tính dựa theo công thức:\n",
    "\n",
    "$$\\det(\\mathbf{M})= \\sum_{\\sigma} \\text{sgn}(\\sigma) m^{1}_{\\sigma(1)}m^{2}_{\\sigma(2)}\\cdots m^{n}_{\\sigma(n)}$$\n",
    "\n",
    "Trong đó $\\sigma=\\{\\sigma(1), \\sigma(2), \\dots, \\sigma(n)\\}$ là một phép hoán vị các thành phần của tập thứ tự ban đầu $O = \\{1, 2, \\dots, n\\}$ và $\\text{sgn}(\\sigma)$ là biểu thức nhận hai gía trị $\\{1, -1\\}$. Nếu số lần hoán vị $\\sigma$ để thu được tập $O$ là chẵn thì nhận gía trị 1 và lẻ thì nhận giá trị -1. \n",
    "\n",
    "Định thức có ý nghĩa rất quan trọng đối với ma trận vì nó cho phép chúng ta biết được các véc tơ cột (hoặc dòng) của ma trận đó có độc lập tuyến tính hay không? Hệ phương trình tạo bởi ma trận đó bao nhiêu nghiệm? Thậm chí chúng ta có thể tính được nghiệm của ma trận theo công thức nghiệm Jacobian.\n",
    "\n",
    "::::{tabbed} pytorch\n",
    ":::{code-block} python\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32)\n",
    "\n",
    "# Định thức của ma trận\n",
    "torch.det(A)\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{tabbed} numpy\n",
    ":::{code-block} python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "# Định thức của ma trận\n",
    "np.linalg.det(A)\n",
    ":::\n",
    "::::\n",
    "\n",
    "Một số tính chất của định thức:\n",
    "\n",
    "Một số tính chất của định thức:\n",
    "\n",
    "1) $\\det(\\mathbf{A}) = \\det (\\mathbf{A}^{\\intercal})$\n",
    "\n",
    "2) Nếu $\\mathbf{A}$ và $\\mathbf{B}$ là những ma trận vuông thì:  $\\det{(\\mathbf{AB})} = \\det({\\mathbf{A}}) \\det({\\mathbf{B}})$\n",
    "\n",
    "3) $\\det(\\mathbf{AB}) = \\det(\\mathbf{BA})$\n",
    "\n",
    "4) Một ma trận đường chéo và vuông thì có định thức bằng tích các phần tử nằm trên đường chéo. $\\det(\\mathbf{A}_{m \\times m}) = a_1.a_2 \\dots a_m$.\n",
    "\n",
    "5) Định thức của ma trận đơn vị thì bằng 1. Tức là: $\\det{(\\mathbf{I}_n)} = 1$\n",
    "\n",
    "6) $\\text{det}(\\mathbf{A}) = \\frac{1}{\\text{det}{(\\mathbf{A}^{-1})}}$\n",
    "\n",
    "7) Một ma trận khả nghịch thì có định thức khác 0. Điều này là hiển nhiên vì giả sử ma trận là khả nghịch và có định thức bằng 0. Khi đó: $\\det{(\\mathbf{A})} \\det{(\\mathbf{A}^{-1})} = 0$. Điều này vô lý vì $\\det{(\\mathbf{A})} \\det{(\\mathbf{A}^{-1})} = 1$ theo như tính chất 6.\n",
    "\n",
    "## 1.9. Tổ hợp tuyến tính và không gian sinh\n",
    "\n",
    "**Hệ véc tơ độc lập tuyến tính là gì?**\n",
    "\n",
    "Một hệ véc tơ $\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n$ là độc lập tuyến tính nếu phương trình:\n",
    "\n",
    "$$k_1 \\mathbf{e}_1 + k_2 \\mathbf{e}_2 + \\dots + k_n \\mathbf{e}_n = 0$$\n",
    "\n",
    "có một nghiệm duy nhất $k_1 = k_2 = \\dots = k_n = 0$\n",
    "\n",
    "Trái lại, nếu tồn tại một nghiệm mà $k_j \\neq 0$ thì hệ véc tơ là phụ thuộc tuyến tính.\n",
    "\n",
    "Khi đó véc tơ $\\mathbf{e}_j$ có thể được biểu diễn thành:\n",
    "\n",
    "$$\\frac{-k_1}{k_j} \\mathbf{e}_1 + \\frac{-k_2}{k_j} \\mathbf{e}_2 + \\dots + \\frac{-k_{j-1}}{k_j} \\mathbf{e}_{j-1} + \\frac{-k_{j+1}}{k_j} \\mathbf{e}_{j+1} + \\dots + \\frac{-k_n}{k_j} \\mathbf{e}_n = \\mathbf{e}_j$$\n",
    "\n",
    "Ta gọi phương trình trên là một tổ hợp tuyến tính của $\\mathbf{e}_j$ theo các véc tơ còn lại.\n",
    "\n",
    "Tập hợp tất cả các véc tơ được tổ hợp tuyến tính từ hệ véc tơ độc lập tuyến tính thì tạo ra một không gian sinh.\n",
    "\n",
    "## 1.10. Biến đổi hệ cơ sở của véc tơ\n",
    "\n",
    "Trong không gian $m$ chiều thì mọi véc tơ đều có thể biểu diễn thông qua hệ véc tơ đơn vị $(\\mathbf{e}_1, \\mathbf{e}_2, \\dots , \\mathbf{e}_m)$. Trong đó véc tơ $\\mathbf{e}_i$ được gọi là véc tơ đơn vị có phần tử thứ $i$ là 1, các phần tử còn lại bằng 0.\n",
    "\n",
    "Bản chất của một phép nhân ma trận với một véc tơ là một phép biến đổi hệ cơ sở mà ở đó mỗi một cột của ma trận được xem như một véc tơ cơ sở. Giả sử ma trận $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ nhân với véc tơ $\\mathbf{x}\\in \\mathbb{R}^{n}$.\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{x} = \\mathbf{a}^{(1)} x_1 + \\mathbf{a}^{(2)} x_2 + \\dots + \\mathbf{a}^{(m)}  x_m = \\mathbf{y}$$\n",
    "\n",
    "Trong đó $\\mathbf{a}^{(i)}$ là một véc tơ cột thứ $i$ của ma trận $\\mathbf{A}$. Các giá trị $x_i$ được xem như toạ độ của $\\mathbf{y}$ trong hệ cơ sở gồm các véc tơ cột của $\\mathbf{A}$.\n",
    "\n",
    "# 2. Tóm tắt\n",
    "\n",
    "Như vậy qua chương này mình đã hướng dẫn cho các bạn các kiến thức bản nhất trong đại số tuyến tính. Bao gồm:\n",
    "\n",
    "1. Các khái niệm: Số vô hướng, véc tơ, ma trận, tensor kèm theo thuộc tính của chúng.\n",
    "2. Các phép tính cộng, trừ, nhân ma trận, nhân véc tơ, nhân ma trận với véc tơ.\n",
    "3. Khái niệm về chuẩn và ý nghĩa của chúng trong vai trò một độ đo đối với véc tơ.\n",
    "\n",
    "Đây là những kiến thức nền tảng nhưng rất quan trọng mà bạn đọc cần nắm vững trước khi học sâu về AI.\n",
    "\n",
    "# 3. Bài tập\n",
    "\n",
    "Một vài bài tập dưới đây sẽ giúp bạn ôn lại kiến thức tốt hơn:\n",
    "\n",
    "1. Khởi tạo một số vô hướng, một véc tơ có độ dài là $3$ và một ma trận bất kỳ có kích thước là $2\\times 3$ trên pytorch.\n",
    "2. Tính tích giữa véc tơ và ma trận.\n",
    "3. Tính tổng các dòng và tổng các cột của ma trận.\n",
    "4. Chứng minh rằng nếu $\\mathbf{A}$ là một ma trận vuông thì $\\mathbf{A} + \\mathbf{A}^{\\top}$ là một ma trận đối xứng.\n",
    "5. Cho $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ là ba ma trận có kích thước lần lượt là $m \\times n$, $n \\times p$ và $p \\times q$ chứng minh rằng $\\mathbf{ABC} = (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})$\n",
    "6. $\\mathbf{trace}$ của ma trận là tổng các phần tử nằm trên đường chéo chính ( phần tử mà có index dòng bằng cột). Chứng minh rằng: $\\mathbf{trace(AB) = trace(BA)}$\n",
    "7. Chứng minh: $\\mathbf{A} \\odot \\mathbf{(B+C)} = \\mathbf{A} \\odot \\mathbf{B} + \\mathbf{A} \\odot \\mathbf{C}$\n",
    "8. Chứng minh: $\\mathbf{A} \\odot (\\mathbf{B} \\odot \\mathbf{C})= (\\mathbf{A} \\odot \\mathbf{B}) \\odot \\mathbf{C}$\n",
    "9. Chứng mình rằng: $$(\\mathbf{A}\\mathbf{B})^{\\intercal} = \\mathbf{B}^{\\intercal}\\mathbf{A}^{\\intercal}$$\n",
    "10. Chứng minh: $$\\mathbf{A}\\mathbf{I} = \\mathbf{A}$$\n",
    "Trong đó $\\mathbf{I}$ là ma trận đơn vị."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.8.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "source_map": [
   11,
   775
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}