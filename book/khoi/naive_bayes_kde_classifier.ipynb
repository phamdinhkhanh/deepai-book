{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Phân Loại Xác Suất (Probabilistic Classifier): Naive Bayes và KDE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Chúng ta bắt đầu bằng 1 bài toán xác suất cơ bản: Một ngày đẹp trời, bạn đi xem thầy bói. Thầy phán rằng bạn sẽ ế trước khi bạn 30. Bạn biết rằng trước đó có 10 người bạn già của bạn cũng đi xem ông này bói, và có 6 người ông ấy phán là sẽ ế. Trong 6 người đó, có 2 người sau này lại có người yêu trước 30, còn 4 người ông ấy phán không ế, có 1 ông tới năm 30 vẫn phải quẹt Tinder. Bạn thắc mắc rằng, khả năng mình ế thật sự sẽ là bao nhiêu phần trăm?\n",
    "\n",
    "Trong các phương pháp để phân loại dữ liệu, sử dụng xác suất là một trong những phương pháp phổ biến nhất. Từ 1 tập dữ liệu có sẵn, ta có thể tính toán ra các xác suất của từng phân loại cho mỗi thực thể T. Và hiển nhiên, các mô hình này sẽ chọn phân loại khả thi nhất cho thực thể đó:\n",
    "\n",
    "$$$\n",
    "class(T) = argmax_{c_i}[P(c_i|T)]\n",
    "$$$\n",
    "\n",
    "\n",
    "Tất nhiên khi xem bói, thì ông thầy bói cũng hỏi bạn đủ thứ số má rồi. Trong những trường hợp 1 thực thể T có nhiều đặc điểm x, thì mô hình sẽ tổng hợp xác suất cho mỗi đặc điểm, rồi cộng lại với nhau:\n",
    "\n",
    "$$$\n",
    "class(T) = argmax_{c_i}\\Big[\\sum_{x_j \\in T} P(c_i|x_j)\\Big]\n",
    "$$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Naive Bayes - BIến rời rạc\n",
    "\n",
    "### 1.1 Mô hình\n",
    "Cách đơn giản nhất để xử lí việc không có đủ dữ liệu trùng khớp để so sánh, đó là tìm ngược lại rằng liệu mỗi phân loại có thể có những đặc điểm trên hay không. Thay vì tìm một người như bạn rồi xem người đó có ế hay không, ông thầy sẽ tìm xem người ế thường giống bạn chỗ nào không. Nói một cách toán học, thì ta sẽ tìm $P(x_j|c_i)$ từ $P(c_i|x_j)$. Nguyên lí Bayes là công thức ta cần để đạt được việc này:\n",
    "\n",
    "$$$\n",
    "P(H|x)=\\frac{P(x|H)P(H)}{P(x)}\n",
    "$$$\n",
    "\n",
    "Những gì ta cần là:\n",
    "* $P(H|x)$ - Những đối tượng có đặc tính $x$ có bao nhiêu phần thuộc phân loại $H$?\n",
    "* $P(H)$ - Phân loại $H$ chiếm bao nhiêu phần trong tập thể?\n",
    "* $P(x)$ - Đặc điểm $x$ chiếm bao nhiêu phần trong tập thể?\n",
    "\n",
    "Bằng công thức này, ta sẽ chọn phân loại có nhiều đặc tính trùng khớp với đối tượng nhất:\n",
    "\n",
    "$$$\n",
    "class(T) = argmax_{c_i}\\Big[\\frac{P(T|c_i)P(c_i)}{P(T)}\\Big]\n",
    "$$$\n",
    "\n",
    "Với nhiều đặc điểm, nguyên lí Bayes sẽ giả định rằng những đặc điểm này độc lập lẫn nhau cho dễ tính, và điều này tạo nên cái tên \"Naive\" (nai tơ). Như vậy, ta có thể sử dụng công thức tính xác suất của các đặc điểm độc lập:\n",
    "\n",
    "$$$\n",
    "P(x_1,x_2,...,x_n|c) = \\prod_{x_i \\in T} P(x_i|c)\n",
    "$$$\n",
    "\n",
    "Bằng cách thay thực thể $T$ bằng 1 dãy đặc điểm $[x_1,x_2,...,x_n]$, công thức sẽ trở thành:\n",
    "\n",
    "$$$\n",
    "class(T) = argmax_{c_i}\\Big[\\frac{P(c_i)\\prod_{x_i \\in T} P(x_i|c)}{P(T)}\\Big]\n",
    "$$$\n",
    "\n",
    "Khoan, dừng lại khoảng chừng là 20 giây! Có thể rút gọn phần nào phương trình này không?\n",
    "\n",
    "Chúng ta có thể để ý rằng dù cho phân loại có là gì, thì xác suất tìm được các đặc tính trong một tập thể $P(T)$ vẫn giữ nguyên và không làm ảnh hưởng đến trật tự của các kết quả. Và vì vậy, con số này không nhất thiết phải mang vào công thức, vì mục đích cuối cùng của mô hình vẫn là so sánh để tìm ra xác suất lớn nhất mà thôi:\n",
    "\n",
    "$$$\n",
    "class(T) = argmax_{c_i}\\Big[P(c_i)\\prod_{x_i \\in T} P(x_i|c)\\Big]\n",
    "$$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Continuous data - discretization\n",
    "* Continuous -> Nominal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Continuous data - Gaussian Naive Bayes\n",
    "* Với dữ liệu continuous, có thể tính xác suất bằng phân khúc Gaussian.\n",
    "* Điều này tất nhiên cũng giả định rằng phân bố dữ liệu cũng là Gaussian."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Continuous data - Kernel Density Estimation\n",
    "* Với dữ liệu không phân bố theo Gaussian, cần có phương pháp phù hợp hơn.\n",
    "* Kernel density tổng hợp tất cả các distribution có cùng standard deviation lại thành một distribution mới.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}